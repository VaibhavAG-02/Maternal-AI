# Maternal-AI ğŸ¤°

An AI-powered maternal health assistant utilizing fine-tuned LLaMA 2 language model to provide guidance and support for expectant mothers.

## ğŸ“‹ Project Overview

Maternal-AI is a comprehensive machine learning project that leverages QLoRA (Quantized Low-Rank Adaptation) fine-tuning to create a specialized AI assistant for maternal health queries. Built on Meta's LLaMA 2-7B model, the system provides accessible, empathetic, and informative responses to pregnancy-related questions through an intuitive Streamlit web interface.

## ğŸ—‚ï¸ Project Structure

```
Maternal-AI/
â”œâ”€â”€ GH_data_preparation.ipynb    # Original Jupyter notebook for data prep
â”œâ”€â”€ GH_qlora.ipynb               # Original QLoRA training notebook
â”œâ”€â”€ GH_Streamlit.ipynb           # Original Streamlit app notebook
â”œâ”€â”€ data_preparation.py          # Converted Python script for data prep
â”œâ”€â”€ qlora_training.py            # Converted QLoRA training script
â”œâ”€â”€ streamlit_app.py             # Converted Streamlit app script
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ .gitignore                   # Git ignore rules
â”œâ”€â”€ README.md                    # Project documentation
â”œâ”€â”€ GITHUB_UPLOAD_GUIDE.md       # Detailed GitHub upload instructions
â””â”€â”€ QUICK_REFERENCE.md           # Quick command reference
```

## ğŸš€ Features

- **Maternal Health Knowledge Base**: Custom-curated dataset of pregnancy-related Q&A
- **QLoRA Fine-tuning**: Memory-efficient 4-bit quantized training on LLaMA 2-7B
- **Interactive Web Interface**: Professional Streamlit application with chat history
- **Human Evaluation System**: Built-in feedback mechanism for response quality
- **Google Drive Integration**: Seamless model and data persistence
- **Emergency Protocol**: Safety features for medical emergencies

## ğŸ“¦ Installation & Setup

### Prerequisites
- Python 3.8 or higher
- CUDA-compatible GPU (T4 or better recommended for training)
- Google Colab account (for original notebooks)
- HuggingFace account with LLaMA 2 access
- Ngrok account (for Streamlit deployment)

### Local Setup

1. **Clone the repository:**
```bash
git clone https://github.com/yourusername/Maternal-AI.git
cd Maternal-AI
```

2. **Create a virtual environment:**
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. **Install dependencies:**
```bash
pip install -r requirements.txt
```

4. **Set up HuggingFace authentication:**
```bash
huggingface-cli login
```
Enter your HuggingFace token when prompted.

## ğŸ’» Usage

### Option 1: Using Jupyter Notebooks (Recommended for Google Colab)

1. **Data Preparation:**
   - Open `GH_data_preparation.ipynb` in Google Colab
   - Follow the cell-by-cell instructions
   - Creates training dataset in Google Drive

2. **Model Training:**
   - Open `GH_qlora.ipynb` in Google Colab
   - Ensure GPU runtime is enabled (Runtime â†’ Change runtime type â†’ GPU)
   - Run all cells to fine-tune LLaMA 2

3. **Deploy Application:**
   - Open `GH_Streamlit.ipynb` in Google Colab
   - Set your Ngrok auth token
   - Run to launch public web interface

### Option 2: Using Python Scripts (For Local Deployment)

1. **Data Preparation:**
```bash
python data_preparation.py
```

2. **Model Training:**
```bash
python qlora_training.py
```

3. **Run Application:**
```bash
streamlit run streamlit_app.py
```

## ğŸ› ï¸ Technologies Used

- **LLaMA 2-7B-chat**: Base language model from Meta AI
- **Transformers**: HuggingFace transformers library
- **PEFT**: Parameter-Efficient Fine-Tuning with LoRA
- **BitsAndBytes**: 4-bit model quantization
- **TRL**: Supervised fine-tuning trainer
- **Streamlit**: Web application framework
- **PyTorch**: Deep learning framework
- **Ngrok**: Public URL tunneling for Colab deployment

## ğŸ“Š Model Details

- **Base Model**: `meta-llama/Llama-2-7b-chat-hf`
- **Fine-tuning Method**: QLoRA (4-bit NF4 quantization)
- **LoRA Configuration**:
  - Rank (r): 16
  - Alpha: 32
  - Target modules: q_proj, k_proj, v_proj, o_proj
  - Dropout: 0.05
- **Training Data**: Custom maternal health Q&A dataset
- **Max Sequence Length**: 512 tokens
- **Task**: Conversational question-answering for maternal health

## ğŸ“Š Model Details

- **Base Model**: `meta-llama/Llama-2-7b-chat-hf`
- **Fine-tuning Method**: QLoRA (4-bit NF4 quantization)
- **LoRA Configuration**:
  - Rank (r): 16
  - Alpha: 32
  - Target modules: q_proj, k_proj, v_proj, o_proj
  - Dropout: 0.05
- **Training Data**: Custom maternal health Q&A dataset
- **Max Sequence Length**: 512 tokens
- **Task**: Conversational question-answering for maternal health

## ğŸ¯ Training Configuration

- **Training Framework**: Supervised Fine-Tuning (SFT) with TRL
- **Optimizer**: Paged AdamW 32-bit
- **Learning Rate**: 2e-4 with cosine scheduler
- **Batch Size**: 4 per device with gradient accumulation
- **Precision**: FP16 mixed precision
- **Hardware**: T4 GPU (15GB VRAM)

## ğŸ”’ Safety & Disclaimer

This AI assistant is designed to provide **general information and support only**.

âš ï¸ **Important**: 
- This is **NOT a substitute for professional medical advice**
- Always consult qualified healthcare providers for medical decisions
- In case of emergency, call your local emergency services immediately
- The app includes built-in emergency protocol detection

## ğŸ“¸ Features Showcase

The Streamlit application includes:
- ğŸ’¬ **Chat Interface**: Natural conversation flow with chat history
- ğŸ“Š **Human Evaluation**: Rate responses for relevance, empathy, and quality
- ğŸ¨ **Professional UI**: Clean, maternal-health themed design
- ğŸ“ **Conversation Logging**: Track interaction history
- ğŸš¨ **Emergency Detection**: Identifies urgent medical situations

## ğŸ”§ Configuration

### For Google Colab Users:

1. **HuggingFace Token**: Store in Colab Secrets as `HF_TOKEN`
2. **Ngrok Token**: Required for public URL (set in notebook)
3. **Google Drive**: Automatically mounts for persistence

### For Local Users:

1. **Model Path**: Update `BEST_MODEL_PATH` in `streamlit_app.py`
2. **Base Model**: Ensure you have LLaMA 2 access approval
3. **GPU**: CUDA-compatible GPU strongly recommended

## ğŸ“ˆ Project Workflow

```
1. Data Preparation
   â”œâ”€â”€ Create knowledge base
   â”œâ”€â”€ Format for instruction tuning
   â””â”€â”€ Save to Google Drive

2. Model Training
   â”œâ”€â”€ Load LLaMA 2-7B with 4-bit quantization
   â”œâ”€â”€ Apply LoRA adapters
   â”œâ”€â”€ Fine-tune with SFT
   â””â”€â”€ Save trained model

3. Deployment
   â”œâ”€â”€ Load fine-tuned model
   â”œâ”€â”€ Create Streamlit interface
   â”œâ”€â”€ Deploy with Ngrok (Colab) or locally
   â””â”€â”€ Enable human evaluation
```

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

### Areas for Contribution:
- Expand the knowledge base with more Q&A pairs
- Improve response quality evaluation metrics
- Add multilingual support
- Enhance UI/UX design
- Add more comprehensive testing

## ğŸ“ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ‘¥ Authors

- Your Name - [Your GitHub Profile]

## ğŸ™ Acknowledgments

- **Meta AI** for LLaMA 2
- **HuggingFace** for transformers and PEFT libraries
- **Tim Dettmers** for bitsandbytes quantization
- **Streamlit** for the amazing web framework
- **Google Colab** for free GPU access

## ğŸ“§ Contact

For questions, suggestions, or feedback:
- Email: [your-email@example.com]
- GitHub Issues: [Link to your repo issues]

## ğŸ“š Resources

- [LLaMA 2 Paper](https://arxiv.org/abs/2307.09288)
- [QLoRA Paper](https://arxiv.org/abs/2305.14314)
- [HuggingFace PEFT Documentation](https://huggingface.co/docs/peft)
- [Streamlit Documentation](https://docs.streamlit.io)

## ğŸ› Known Issues

- Colab sessions timeout after inactivity - save work frequently
- Large model downloads require stable internet connection
- Ngrok free tier has session limits

## ğŸ”® Future Enhancements

- [ ] Add RAG (Retrieval Augmented Generation) for medical references
- [ ] Implement conversation summarization
- [ ] Add voice input/output capabilities
- [ ] Create mobile-friendly version
- [ ] Integrate with health tracking APIs
- [ ] Multi-model comparison interface
- [ ] Automated evaluation metrics

---

**Note**: Remember to:
1. Replace placeholder text (GitHub username, email, etc.) with your actual information
2. Request access to LLaMA 2 on HuggingFace before training
3. Never commit API keys or tokens to the repository
4. Review all outputs from the AI for medical accuracy

**Made with â¤ï¸ for maternal health**
